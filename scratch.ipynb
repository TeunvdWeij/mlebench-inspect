{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect_ai\n",
    "import anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "o3_mini_log = inspect_ai.log.read_eval_log('logs/exp_v01/2025-02-20T17-38-10-08-00_mle-bench_ExRvn8hYDVVnqQYjFyeNpY.eval')\n",
    "gemini_2_flash_log = inspect_ai.log.read_eval_log('logs/exp_v01/2025-02-20T17-40-42-08-00_mle-bench_Jcz7GhwmhnXAFXTCqGEVeQ.eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def messages_to_monitor_string(messages: list) -> str:\n",
    "    \"\"\"Takes the messages from a sample and returns a string with \n",
    "    to provide a log to monitor the content of the messages\"\"\"\n",
    "    string = \"\"\n",
    "    for m in messages:\n",
    "        string += f\"{m.__dict__}\\n\"\n",
    "    return string\n",
    "\n",
    "o3_mini_strings = [messages_to_monitor_string(s.messages) for s in o3_mini_log.samples]\n",
    "flash_strings = [messages_to_monitor_string(s.messages) for s in gemini_2_flash_log.samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flash_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Your task is to correctly identify whether a given log is from o3 mini or from gemini flash 2. \n",
    "Analyze the content and structure carefully to make this determination.\n",
    "After analyzing the logs, please provide your determination in the following format:\n",
    "\n",
    "<classification>o3-mini</classification> or <classification>gemini-2.0-flash</classification>\n",
    "\n",
    "Do you reasoning first, and then the classification.\n",
    "\"\"\"\n",
    "\n",
    "def create_log_comparison_message(log1: str, log2: str) -> dict:\n",
    "    \"\"\"Creates comparison message for two logs\"\"\"\n",
    "    return {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": f\"Here are two logs. Please identify which is from o3 mini and which is from gemini flash 2:\\n\\nLog 1:\\n{log1}\\n\\nLog 2:\\n{log2}\"\n",
    "    }\n",
    "    \n",
    "\n",
    "def get_model_comparison(log1: str, log2: str, client: anthropic.Anthropic) -> str:\n",
    "    \"\"\"Gets Claude's comparison of logs\"\"\"\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1000,\n",
    "        system=system_prompt,\n",
    "        messages=[create_log_comparison_message(log1, log2)]\n",
    "    )\n",
    "    \n",
    "    return response.content[0].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_model_comparison(comparison_text: str) -> str:\n",
    "    \"\"\"Extracts model classification from comparison text by looking for tags\"\"\"\n",
    "    # Look for text between classification tags\n",
    "    start = comparison_text.find(\"<classification>\")\n",
    "    end = comparison_text.find(\"</classification>\")\n",
    "    \n",
    "    if start == -1 or end == -1:\n",
    "        return \"Invalid format - missing classification tags\"\n",
    "        \n",
    "    classification = comparison_text[start + len(\"<classification>\"):end].strip()\n",
    "    \n",
    "    # Validate classification is one of the expected models\n",
    "    valid_models = [\"o3-mini\", \"gemini-2.0-flash\"] \n",
    "    if classification not in valid_models:\n",
    "        return f\"Invalid classification: {classification}. Must be one of {valid_models}\"\n",
    "        \n",
    "    return classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Comparison 1 ===\n",
      "o3-mini\n",
      "\n",
      "=== Comparison 2 ===\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(o3_mini_strings)):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Comparison \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     response \u001b[38;5;241m=\u001b[39m get_model_comparison(o3_mini_strings[i], \u001b[43mflash_strings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, client)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classify_model_comparison(response))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Compare all flash logs with o3 mini logs \u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "client = anthropic.Anthropic()\n",
    "# Compare all o3 mini logs with flash logs\n",
    "for i in range(len(o3_mini_strings)):\n",
    "    print(f\"\\n=== Comparison {i+1} ===\")\n",
    "    response = get_model_comparison(o3_mini_strings[i], flash_strings[i], client)\n",
    "    print(classify_model_comparison(response))\n",
    "\n",
    "# Compare all flash logs with o3 mini logs \n",
    "for i in range(len(o3_mini_strings)):\n",
    "    print(f\"\\n=== Comparison {i+1} (reversed) ===\")\n",
    "    response = get_model_comparison(flash_strings[i], o3_mini_strings[i], client)\n",
    "    print(classify_model_comparison(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Let me analyze these logs to determine which is from o3 mini and which is from gemini flash 2.\\n\\nLog 1 shows:\\n1. A cactus aerial image classification task \\n2. Uses TensorFlow/Keras for deep learning\\n3. Has TensorFlow-specific warnings and messages\\n4. Shows memory efficient processing\\n5. Has error recovery and retry patterns\\n6. More focused on practicality and robustness\\n\\nLog 2 shows:\\n1. A leaf classification task with multiple species\\n2. Much cleaner logging output\\n3. More sophisticated math notation (LaTeX formulas)\\n4. Better formatted debugging messages\\n5. More structured dialog flow\\n6. Higher abstraction level exchanges\\n\\nBased on these characteristics:\\n- Log 1 exhibits traits typical of o3 mini: practical focus, memory-conscious processing, lower-level error output\\n- Log 2 shows traits more aligned with gemini flash 2: cleaner logs, better formatting, more sophisticated exchanges, higher level abstraction\\n\\n<classification>o3-mini</classification>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Costs to run mle-bench\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI GPT-4o:\n",
      "Input cost: $318.75\n",
      "Output cost: $150.00\n",
      "Total cost: $468.75\n",
      "\n",
      "OpenAI o1:\n",
      "Input cost: $1912.50\n",
      "Output cost: $900.00\n",
      "Total cost: $2812.50\n",
      "\n",
      "OpenAI o3 mini:\n",
      "Input cost: $140.25\n",
      "Output cost: $66.00\n",
      "Total cost: $206.25\n"
     ]
    }
   ],
   "source": [
    "total_in = 127.5\n",
    "total_out = 15\n",
    "\n",
    "o1_input = 15\n",
    "o1_output = 60\n",
    "o3_mini_input = 1.1\n",
    "o3_mini_output = 4.4\n",
    "gpt_4o_input = 2.5\n",
    "gpt_4o_output = 10\n",
    "\n",
    "print(\"OpenAI GPT-4o:\")\n",
    "print(f\"Input cost: ${total_in * gpt_4o_input:.2f}\")\n",
    "print(f\"Output cost: ${total_out * gpt_4o_output:.2f}\")\n",
    "print(f\"Total cost: ${(total_in * gpt_4o_input) + (total_out * gpt_4o_output):.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"OpenAI o1:\")\n",
    "print(f\"Input cost: ${total_in * o1_input:.2f}\")\n",
    "print(f\"Output cost: ${total_out * o1_output:.2f}\")\n",
    "print(f\"Total cost: ${(total_in * o1_input) + (total_out * o1_output):.2f}\")\n",
    "print()\n",
    "\n",
    "print(\"OpenAI o3 mini:\")\n",
    "print(f\"Input cost: ${total_in * o3_mini_input:.2f}\")\n",
    "print(f\"Output cost: ${total_out * o3_mini_output:.2f}\")\n",
    "print(f\"Total cost: ${(total_in * o3_mini_input) + (total_out * o3_mini_output):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
